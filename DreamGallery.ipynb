{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "DreamGallery.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŒŸ DreamGallery: AIâ€‘Powered Artwork Generator\n",
        "\n",
        "This notebook implements the **DreamGallery** pipeline:\n",
        "1. Setup Colab environment (GPU, Drive mount, installs)\n",
        "2. Download & preprocess WikiArt dataset\n",
        "3. Define and train a GAN to generate base images\n",
        "4. Apply neural style transfer to stylize generated art\n",
        "5. Generate samples, visualize, and save outputs\n",
        "6. Commit code & results back to GitHub from Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Enable GPU, mount your Google Drive, and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "colab": { "base_uri": "https://localhost:8080/" } },
      "source": [
        "# 1.1 Check GPU availability\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1.2 Mount Google Drive for persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1.3 Install required packages\n",
        "!pip install --quiet numpy matplotlib tensorflow opencv-python scikit-learn kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Download the Painter by Numbers dataset via the Kaggle API and preprocess images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Configure Kaggle API\n",
        "Upload your `kaggle.json` credential file under `~/.kaggle/` before running."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2.1.1 Create kaggle folder & copy credentials (run only if needed)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Download & extract dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 2.2.1 Download Painter by Numbers dataset\n",
        "!kaggle competitions download -c painter-by-numbers -p data/raw\n",
        "\n",
        "# 2.2.2 Unzip images\n",
        "!unzip -q data/raw/*.zip -d data/raw/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Preprocess images (resize + normalize)\n",
        "Create `data/processed/128x128/` with `.npy` arrays for fast loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, cv2, numpy as np\n",
        "\n",
        "RAW_DIR = 'data/raw/train'\n",
        "PROC_DIR = 'data/processed/128x128'\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "\n",
        "def preprocess_and_save(src_dir, dst_dir, size=(128,128)):\n",
        "    for fname in os.listdir(src_dir):\n",
        "        path = os.path.join(src_dir, fname)\n",
        "        img = cv2.imread(path)\n",
        "        if img is None: continue\n",
        "        img = cv2.resize(img, size)\n",
        "        img = img.astype('float32') / 255.0\n",
        "        np.save(os.path.join(dst_dir, fname.split('.')[0] + '.npy'), img)\n",
        "\n",
        "preprocess_and_save(RAW_DIR, PROC_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick check of processed data shape\n",
        "arr = np.load(os.path.join(PROC_DIR, os.listdir(PROC_DIR)[0]))\n",
        "print('Sample shape:', arr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. GAN Model Definition\n",
        "Define generator, discriminator, and training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "LATENT_DIM = 100\n",
        "IMG_SHAPE = (128,128,3)\n",
        "\n",
        "def build_generator(latent_dim=LATENT_DIM):\n",
        "    inp = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(16*16*128, activation='relu')(inp)\n",
        "    x = layers.Reshape((16,16,128))(x)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    x = layers.Conv2D(128,3,padding='same', activation='relu')(x)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    x = layers.Conv2D(64,3,padding='same', activation='relu')(x)\n",
        "    out = layers.Conv2D(3,3,padding='same', activation='tanh')(x)\n",
        "    return Model(inp, out, name='Generator')\n",
        "\n",
        "def build_discriminator(img_shape=IMG_SHAPE):\n",
        "    inp = layers.Input(shape=img_shape)\n",
        "    x = layers.Conv2D(64,3,strides=2,padding='same')(inp)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Conv2D(128,3,strides=2,padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inp, out, name='Discriminator')\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "generator.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. GAN Training Loop\n",
        "Load processed images, compile models, and train the GAN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import glob\nn",
        "def load_processed(dir_path):\n",
        "    files = glob.glob(f'{dir_path}/*.npy')\n",
        "    data = [np.load(fp) for fp in files]\n",
        "    return np.array(data)\n",
        "\n",
        "# Load data\n",
        "images = load_processed(PROC_DIR)\n",
        "print('Dataset size:', images.shape)\n",
        "\n",
        "# Labels for real/fake\n",
        "real_labels = np.ones((images.shape[0], 1))\n",
        "fake_labels = np.zeros((images.shape[0], 1))\n",
        "\n",
        "# Compile discriminator\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build combined model\n",
        "discriminator.trainable = False\n",
        "z = layers.Input(shape=(LATENT_DIM,))\n",
        "img = generator(z)\n",
        "validity = discriminator(img)\n",
        "combined = Model(z, validity)\n",
        "combined.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 5000\n",
        "BATCH = 64\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    idx = np.random.randint(0, images.shape[0], BATCH)\n",
        "    real_imgs = images[idx]\n",
        "    noise = np.random.normal(0, 1, (BATCH, LATENT_DIM))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train on real and fake\n",
        "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((BATCH,1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((BATCH,1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train generator\n",
        "    g_loss = combined.train_on_batch(noise, np.ones((BATCH,1)))\n",
        "\n",
        "    # Log every 500 epochs\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} / {EPOCHS}  [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%]  [G loss: {g_loss:.4f}]\")\n",
        "\n",
        "    # Save model checkpoints\n",
        "    if epoch % 1000 == 0:\n",
        "        generator.save(f'results/gan_generator_epoch{epoch}.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Neural Style Transfer\n",
        "Use TensorFlow's pretrained VGG19 to apply style transfer to GAN outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def load_and_process(path, target_size=(128,128)):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.resize(img, target_size)\n",
        "    img = img[tf.newaxis, ...]\n",
        "    return vgg19.preprocess_input(img*255.0)\n",
        "\n",
        "def deprocess(x):\n",
        "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    x = x[:, :, ::-1]\n",
        "    return tf.clip_by_value(x, 0, 255) / 255.0\n",
        "\n",
        "# Load VGG19 for style and content\n",
        "vgg = vgg19.VGG19(weights='imagenet', include_top=False)\n",
        "style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1']\n",
        "content_layers = ['block5_conv2']\n",
        "outputs = [vgg.get_layer(name).output for name in (style_layers + content_layers)]\n",
        "style_model = Model(vgg.input, outputs)\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    channels = int(tensor.shape[-1])\n",
        "    a = tf.reshape(tensor, [-1, channels])\n",
        "    return tf.matmul(a, a, transpose_a=True)\n",
        "\n",
        "def compute_loss(comb, content, style):\n",
        "    comb_feats = style_model(comb)\n",
        "    style_feats = style_model(style)\n",
        "    content_feats = style_model(content)\n",
        "\n",
        "    # Content loss\n",
        "    c_loss = tf.reduce_mean((comb_feats[-1] - content_feats[-1])**2)\n",
        "\n",
        "    # Style loss\n",
        "    s_loss = 0\n",
        "    weight_per_style = 1.0 / len(style_layers)\n",
        "    for cf, sf in zip(comb_feats[:len(style_layers)], style_feats[:len(style_layers)]):\n",
        "        s_loss += weight_per_style * tf.reduce_mean((gram_matrix(cf) - gram_matrix(sf))**2)\n",
        "\n",
        "    return c_loss + 1e-2 * s_loss\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=5.0)\n",
        "\n",
        "def style_transfer(content_path, style_path, iterations=200):\n",
        "    content = load_and_process(content_path)\n",
        "    style = load_and_process(style_path)\n",
        "    comb = tf.Variable(content)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = compute_loss(comb, content, style)\n",
        "        grads = tape.gradient(loss, comb)\n",
        "        optimizer.apply_gradients([(grads, comb)])\n        \n",
        "    return deprocess(comb.numpy())  # final stylized image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inference & Visualization\n",
        "Generate new images with the trained GAN, apply style transfer, and display."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 6.1 Generate base art\n",
        "noise = np.random.normal(size=(1, LATENT_DIM))\n",
        "gen_img = generator.predict(noise)\n",
        "gen_img = (gen_img[0] * 0.5) + 0.5  # rescale from [-1,1] to [0,1]\n",
        "\n",
        "# Save temporarily\n",
        "cv2.imwrite('results/base_art.jpg', (gen_img*255).astype('uint8')[...,::-1])\n",
        "\n",
        "# 6.2 Apply style transfer\n",
        "stylized = style_transfer('results/base_art.jpg', 'path/to/your/style.jpg', iterations=100)\n",
        "\n",
        "# 6.3 Display results\n",
        "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
        "axes[0].imshow(gen_img); axes[0].set_title('GAN Output'); axes[0].axis('off')\n",
        "axes[1].imshow(stylized); axes[1].set_title('Stylized Output'); axes[1].axis('off')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Models & Results to Drive\n",
        "Persist your trained generator and final outputs to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7.1 Save model\n",
        "generator.save('/content/drive/MyDrive/dreamgallery/results/gan_generator_final.h5')\n",
        "\n",
        "# 7.2 Save stylized image\n",
        "import imageio\n",
        "imageio.imwrite('/content/drive/MyDrive/dreamgallery/results/stylized_final.jpg', (stylized*255).astype('uint8'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Commit & Push to GitHub\n",
        "Use Colabâ€™s shell to push your updated code and results back to your GitHub repo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure Git (first-time only)\n",
        "!git config --global user.name 'Your Name'\n",
        "!git config --global user.email 'you@example.com'\n",
        "\n",
        "# Stage, commit, and push\n",
        "!git add .\n",
        "!git commit -m 'Add trained GAN model and generated artwork'\n",
        "!git push https://<YOUR_TOKEN>@github.com/yourusername/dreamgallery.git HEAD:main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Congrats!** Youâ€™ve run the entire DreamGallery pipeline in Colabâ€”from raw data to stylized masterpiecesâ€”\n",
        "and saved everything to both Google Drive and GitHub. Feel free to fork, modify, and share!"
      ]
    }
  ]
}
