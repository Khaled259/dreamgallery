{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "DreamGallery.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jssdPdpwFWt9"
      },
      "source": [
        "# ğŸŒŸ DreamGallery: AIâ€‘Powered Artwork Generator\n",
        "\n",
        "This notebook implements the **DreamGallery** pipeline:\n",
        "1. Setup Colab environment (GPU, Drive mount, installs)\n",
        "2. Download & preprocess WikiArt dataset\n",
        "3. Define and train a GAN to generate base images\n",
        "4. Apply neural style transfer to stylize generated art\n",
        "5. Generate samples, visualize, and save outputs\n",
        "6. Commit code & results back to GitHub from Colab\n"
      ],
      "id": "jssdPdpwFWt9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCXGYbZFWuA"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Enable GPU, mount your Google Drive, and install dependencies."
      ],
      "id": "RNCXGYbZFWuA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9y0ADn_FWuB",
        "outputId": "e1e9bed4-07e1-47d3-ab9a-042628d215c8"
      },
      "source": [
        "# 1.1 Check GPU availability\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "id": "I9y0ADn_FWuB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj-oi2vEFWuE",
        "outputId": "36beb2b2-e1f6-49d0-a3e5-f1d30a33665d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 1.2 Mount Google Drive for persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "id": "gj-oi2vEFWuE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oehEusCXFWuE"
      },
      "source": [
        "# 1.3 Install required packages\n",
        "!pip install --quiet numpy matplotlib tensorflow opencv-python scikit-learn kaggle"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "oehEusCXFWuE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDJXeOPLFWuF"
      },
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "Download the Painter by Numbers dataset via the Kaggle API and preprocess images."
      ],
      "id": "GDJXeOPLFWuF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf9y6sz-FWuF"
      },
      "source": [
        "### 2.1 Configure Kaggle API\n",
        "Upload your `kaggle.json` credential file under `~/.kaggle/` before running."
      ],
      "id": "Yf9y6sz-FWuF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG4241F-FWuG"
      },
      "source": [
        "# 2.1.1 Create kaggle folder & copy credentials (run only if needed)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 6,
      "outputs": [],
      "id": "FG4241F-FWuG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6n36vnFWuG"
      },
      "source": [
        "### 2.2 Download & extract dataset"
      ],
      "id": "__6n36vnFWuG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC4bruB5FWuI",
        "outputId": "beb0f898-b7ad-4cbd-cb5d-e5ce4a260b9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 2.2.1 Download The Met Open Access dataset via Kaggle CLI\n",
        "!kaggle datasets download \\\n",
        "    -d metmuseum/the-metropolitan-museum-of-art-open-access \\\n",
        "    -p data/raw\n",
        "\n",
        "# 2.2.2 Unzip all contents into data/raw/\n",
        "!unzip -q data/raw/the-metropolitan-museum-of-art-open-access.zip \\\n",
        "    -d data/raw\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/metmuseum/the-metropolitan-museum-of-art-open-access\n",
            "License(s): CC0-1.0\n"
          ]
        }
      ],
      "id": "QC4bruB5FWuI"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2.2 Unzip all contents into data/raw/\n",
        "!unzip -q data/raw/the-metropolitan-museum-of-art-open-access.zip \\\n",
        "    -d data/raw\n"
      ],
      "metadata": {
        "id": "pGXjKADLJOXP",
        "outputId": "e603aaaa-5074-4e9a-da0b-3e7305e58e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pGXjKADLJOXP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace data/raw/MetObjects.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/data/raw"
      ],
      "metadata": {
        "id": "F_mkOSB4JZm8",
        "outputId": "cda92fd5-0563-40ce-a9e1-80c7cde7196a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "F_mkOSB4JZm8",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/content/data/raw': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths\n",
        "CSV_PATH = 'data/raw/MetObjects.csv'\n",
        "IMG_DIR  = 'data/raw/images'\n",
        "os.makedirs(IMG_DIR, exist_ok=True)\n",
        "\n",
        "# 1. Load metadata, suppressing the dtype warning\n",
        "meta = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "\n",
        "# 2. Filter to CC0 paintings\n",
        "meta = meta[meta['Is Public Domain'] == True]\n",
        "meta = meta[meta['Medium'].str.contains('paint', case=False, na=False)]\n",
        "\n",
        "# 3. Helper: fetch image URL from The Met API\n",
        "API_URL = 'https://collectionapi.metmuseum.org/public/collection/v1/objects/{}'\n",
        "def fetch_image_url(object_id):\n",
        "    res = requests.get(API_URL.format(object_id), timeout=5)\n",
        "    if not res.ok:\n",
        "        return None\n",
        "    data = res.json()\n",
        "    # Prefer smaller, webâ€‘sized image for speed\n",
        "    return data.get('primaryImageSmall') or data.get('primaryImage')\n",
        "\n",
        "# 4. Download loop\n",
        "failed = []\n",
        "for object_id in tqdm(meta['Object ID'].unique(), total=meta['Object ID'].nunique()):\n",
        "    out_path = os.path.join(IMG_DIR, f\"{object_id}.jpg\")\n",
        "    if os.path.exists(out_path):\n",
        "        continue  # already downloaded\n",
        "\n",
        "    url = fetch_image_url(object_id)\n",
        "    if not url:\n",
        "        failed.append(object_id)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=10)\n",
        "        resp.raise_for_status()\n",
        "        with open(out_path, 'wb') as f:\n",
        "            f.write(resp.content)\n",
        "    except Exception as e:\n",
        "        failed.append(object_id)\n",
        "    time.sleep(0.01)  # throttle to avoid rateâ€‘limit\n",
        "\n",
        "# 5. Summary\n",
        "print(f\"Downloaded {len(os.listdir(IMG_DIR))} images, {len(failed)} failures.\")\n"
      ],
      "metadata": {
        "id": "alMwEaGUKn-p",
        "outputId": "571526da-7cf0-482c-f20f-afad3d8d8ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "alMwEaGUKn-p",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8048/8048 [56:23<00:00,  2.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded 7365 images, 683 failures.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYwG8-5wFWuJ"
      },
      "source": [
        "### 2.3 Preprocess images (resize + normalize)\n",
        "Create `data/processed/128x128/` with `.npy` arrays for fast loading."
      ],
      "id": "xYwG8-5wFWuJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7yG29WvFWuJ",
        "outputId": "d6edf4db-664e-47be-ef5d-c54f3ed7db8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "import os, cv2, numpy as np\n",
        "\n",
        "RAW_DIR = 'data/raw/train'\n",
        "PROC_DIR = 'data/processed/128x128'\n",
        "os.makedirs(PROC_DIR, exist_ok=True)\n",
        "\n",
        "def preprocess_and_save(src_dir, dst_dir, size=(128,128)):\n",
        "    for fname in os.listdir(src_dir):\n",
        "        path = os.path.join(src_dir, fname)\n",
        "        img = cv2.imread(path)\n",
        "        if img is None: continue\n",
        "        img = cv2.resize(img, size)\n",
        "        img = img.astype('float32') / 255.0\n",
        "        np.save(os.path.join(dst_dir, fname.split('.')[0] + '.npy'), img)\n",
        "\n",
        "preprocess_and_save(RAW_DIR, PROC_DIR)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/raw/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9c73c4269ba7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpreprocess_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAW_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROC_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-9c73c4269ba7>\u001b[0m in \u001b[0;36mpreprocess_and_save\u001b[0;34m(src_dir, dst_dir, size)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/train'"
          ]
        }
      ],
      "id": "l7yG29WvFWuJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei2V0ASSFWuK"
      },
      "source": [
        "# Quick check of processed data shape\n",
        "arr = np.load(os.path.join(PROC_DIR, os.listdir(PROC_DIR)[0]))\n",
        "print('Sample shape:', arr.shape)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ei2V0ASSFWuK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXvuECCxFWuL"
      },
      "source": [
        "## 3. GAN Model Definition\n",
        "Define generator, discriminator, and training loop."
      ],
      "id": "oXvuECCxFWuL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHAM7gorFWuM",
        "outputId": "d03ff56b-f18c-4b91-c39f-0a8ebcaacd3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "LATENT_DIM = 100\n",
        "IMG_SHAPE = (128,128,3)\n",
        "\n",
        "def build_generator(latent_dim=LATENT_DIM):\n",
        "    inp = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(16*16*128, activation='relu')(inp)\n",
        "    x = layers.Reshape((16,16,128))(x)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    x = layers.Conv2D(128,3,padding='same', activation='relu')(x)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "    x = layers.Conv2D(64,3,padding='same', activation='relu')(x)\n",
        "    out = layers.Conv2D(3,3,padding='same', activation='tanh')(x)\n",
        "    return Model(inp, out, name='Generator')\n",
        "\n",
        "def build_discriminator(img_shape=IMG_SHAPE):\n",
        "    inp = layers.Input(shape=img_shape)\n",
        "    x = layers.Conv2D(64,3,strides=2,padding='same')(inp)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Conv2D(128,3,strides=2,padding='same')(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inp, out, name='Discriminator')\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "generator.summary()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32768\u001b[0m)          â”‚     \u001b[38;5;34m3,309,568\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ reshape (\u001b[38;5;33mReshape\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚       \u001b[38;5;34m147,584\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m73,792\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      â”‚         \u001b[38;5;34m1,731\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32768</span>)          â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,309,568</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,792</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,731</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,532,675\u001b[0m (13.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,532,675</span> (13.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,532,675\u001b[0m (13.48 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,532,675</span> (13.48 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "id": "yHAM7gorFWuM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQhVSH-AFWuM"
      },
      "source": [
        "## 4. GAN Training Loop\n",
        "Load processed images, compile models, and train the GAN."
      ],
      "id": "IQhVSH-AFWuM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlU97ha3FWuM"
      },
      "source": [
        "import glob\n",
        "ndef load_processed(dir_path):\n",
        "    files = glob.glob(f'{dir_path}/*.npy')\n",
        "    data = [np.load(fp) for fp in files]\n",
        "    return np.array(data)\n",
        "\n",
        "# Load data\n",
        "images = load_processed(PROC_DIR)\n",
        "print('Dataset size:', images.shape)\n",
        "\n",
        "# Labels for real/fake\n",
        "real_labels = np.ones((images.shape[0], 1))\n",
        "fake_labels = np.zeros((images.shape[0], 1))\n",
        "\n",
        "# Compile discriminator\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Build combined model\n",
        "discriminator.trainable = False\n",
        "z = layers.Input(shape=(LATENT_DIM,))\n",
        "img = generator(z)\n",
        "validity = discriminator(img)\n",
        "combined = Model(z, validity)\n",
        "combined.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 5000\n",
        "BATCH = 64\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    idx = np.random.randint(0, images.shape[0], BATCH)\n",
        "    real_imgs = images[idx]\n",
        "    noise = np.random.normal(0, 1, (BATCH, LATENT_DIM))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Train on real and fake\n",
        "    d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((BATCH,1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((BATCH,1)))\n",
        "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    # Train generator\n",
        "    g_loss = combined.train_on_batch(noise, np.ones((BATCH,1)))\n",
        "\n",
        "    # Log every 500 epochs\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} / {EPOCHS}  [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%]  [G loss: {g_loss:.4f}]\")\n",
        "\n",
        "    # Save model checkpoints\n",
        "    if epoch % 1000 == 0:\n",
        "        generator.save(f'results/gan_generator_epoch{epoch}.h5')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "TlU97ha3FWuM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvHjCwh_FWuN"
      },
      "source": [
        "## 5. Neural Style Transfer\n",
        "Use TensorFlow's pretrained VGG19 to apply style transfer to GAN outputs."
      ],
      "id": "kvHjCwh_FWuN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cNePOasFWuO"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def load_and_process(path, target_size=(128,128)):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.resize(img, target_size)\n",
        "    img = img[tf.newaxis, ...]\n",
        "    return vgg19.preprocess_input(img*255.0)\n",
        "\n",
        "def deprocess(x):\n",
        "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    x = x[:, :, ::-1]\n",
        "    return tf.clip_by_value(x, 0, 255) / 255.0\n",
        "\n",
        "# Load VGG19 for style and content\n",
        "vgg = vgg19.VGG19(weights='imagenet', include_top=False)\n",
        "style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1']\n",
        "content_layers = ['block5_conv2']\n",
        "outputs = [vgg.get_layer(name).output for name in (style_layers + content_layers)]\n",
        "style_model = Model(vgg.input, outputs)\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    channels = int(tensor.shape[-1])\n",
        "    a = tf.reshape(tensor, [-1, channels])\n",
        "    return tf.matmul(a, a, transpose_a=True)\n",
        "\n",
        "def compute_loss(comb, content, style):\n",
        "    comb_feats = style_model(comb)\n",
        "    style_feats = style_model(style)\n",
        "    content_feats = style_model(content)\n",
        "\n",
        "    # Content loss\n",
        "    c_loss = tf.reduce_mean((comb_feats[-1] - content_feats[-1])**2)\n",
        "\n",
        "    # Style loss\n",
        "    s_loss = 0\n",
        "    weight_per_style = 1.0 / len(style_layers)\n",
        "    for cf, sf in zip(comb_feats[:len(style_layers)], style_feats[:len(style_layers)]):\n",
        "        s_loss += weight_per_style * tf.reduce_mean((gram_matrix(cf) - gram_matrix(sf))**2)\n",
        "\n",
        "    return c_loss + 1e-2 * s_loss\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=5.0)\n",
        "\n",
        "def style_transfer(content_path, style_path, iterations=200):\n",
        "    content = load_and_process(content_path)\n",
        "    style = load_and_process(style_path)\n",
        "    comb = tf.Variable(content)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = compute_loss(comb, content, style)\n",
        "        grads = tape.gradient(loss, comb)\n",
        "        optimizer.apply_gradients([(grads, comb)])\n",
        "\n",
        "    return deprocess(comb.numpy())  # final stylized image"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "0cNePOasFWuO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ8XNSzKFWuP"
      },
      "source": [
        "## 6. Inference & Visualization\n",
        "Generate new images with the trained GAN, apply style transfer, and display."
      ],
      "id": "oQ8XNSzKFWuP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUSHGkx8FWuP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 6.1 Generate base art\n",
        "noise = np.random.normal(size=(1, LATENT_DIM))\n",
        "gen_img = generator.predict(noise)\n",
        "gen_img = (gen_img[0] * 0.5) + 0.5  # rescale from [-1,1] to [0,1]\n",
        "\n",
        "# Save temporarily\n",
        "cv2.imwrite('results/base_art.jpg', (gen_img*255).astype('uint8')[...,::-1])\n",
        "\n",
        "# 6.2 Apply style transfer\n",
        "stylized = style_transfer('results/base_art.jpg', 'path/to/your/style.jpg', iterations=100)\n",
        "\n",
        "# 6.3 Display results\n",
        "fig, axes = plt.subplots(1,2, figsize=(10,5))\n",
        "axes[0].imshow(gen_img); axes[0].set_title('GAN Output'); axes[0].axis('off')\n",
        "axes[1].imshow(stylized); axes[1].set_title('Stylized Output'); axes[1].axis('off')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "DUSHGkx8FWuP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54h7g7ZeFWuQ"
      },
      "source": [
        "## 7. Save Models & Results to Drive\n",
        "Persist your trained generator and final outputs to Google Drive."
      ],
      "id": "54h7g7ZeFWuQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jho5pAiFWuQ"
      },
      "source": [
        "# 7.1 Save model\n",
        "generator.save('/content/drive/MyDrive/dreamgallery/results/gan_generator_final.h5')\n",
        "\n",
        "# 7.2 Save stylized image\n",
        "import imageio\n",
        "imageio.imwrite('/content/drive/MyDrive/dreamgallery/results/stylized_final.jpg', (stylized*255).astype('uint8'))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3Jho5pAiFWuQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5ZnI1DqFWuQ"
      },
      "source": [
        "## 8. Commit & Push to GitHub\n",
        "Use Colabâ€™s shell to push your updated code and results back to your GitHub repo."
      ],
      "id": "C5ZnI1DqFWuQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKc-b3w1FWuR"
      },
      "source": [
        "# Configure Git (first-time only)\n",
        "!git config --global user.name 'Your Name'\n",
        "!git config --global user.email 'you@example.com'\n",
        "\n",
        "# Stage, commit, and push\n",
        "!git add .\n",
        "!git commit -m 'Add trained GAN model and generated artwork'\n",
        "!git push https://<YOUR_TOKEN>@github.com/yourusername/dreamgallery.git HEAD:main"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "FKc-b3w1FWuR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8X2bQqlFWuS"
      },
      "source": [
        "---\n",
        "**Congrats!** Youâ€™ve run the entire DreamGallery pipeline in Colabâ€”from raw data to stylized masterpiecesâ€”\n",
        "and saved everything to both Google Drive and GitHub. Feel free to fork, modify, and share!"
      ],
      "id": "Q8X2bQqlFWuS"
    }
  ]
}